{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ba09b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ea80e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1d2f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_folders(directory):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"CK+48\"\n",
    "folders_list = list_folders(directory_path)\n",
    "print(folders_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48354e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dataset Preparation\n",
    "dataset_dir = 'CK+48'  # Path to CK+ dataset directory\n",
    "\n",
    "# Define the list of emotions\n",
    "# emotions = ['anger', 'fear', 'happy', 'sadness', 'disgust']\n",
    "emotions = folders_list\n",
    "\n",
    "\n",
    "# Define the image width, height, and channels\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "img_channels = 1\n",
    "\n",
    "# Initialize empty lists to store images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load the images and labels\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(dataset_dir, emotion)\n",
    "    for image_name in os.listdir(emotion_dir):\n",
    "        image_path = os.path.join(emotion_dir, image_name)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (img_width, img_height))  # Resize the image\n",
    "        images.append(image)\n",
    "        labels.append(emotion)\n",
    "\n",
    "# Convert the image and label lists to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).unsqueeze(1).float()\n",
    "X_test = torch.from_numpy(X_test).unsqueeze(1).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37766b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Architecture (CNN)\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(img_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * (img_width // 4) * (img_height // 4), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "model = EmotionModel(len(emotions))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a494c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "num_train_samples = X_train.shape[0]\n",
    "num_batches = num_train_samples // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(num_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = X_train[start:end]\n",
    "        labels = y_train[start:end]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddd8ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00        23\n",
      "    contempt       0.00      0.00      0.00         9\n",
      "     disgust       0.00      0.00      0.00        43\n",
      "        fear       0.00      0.00      0.00        15\n",
      "       happy       0.00      0.00      0.00        43\n",
      "     sadness       0.00      0.00      0.00        19\n",
      "    surprise       0.23      1.00      0.37        45\n",
      "\n",
      "    accuracy                           0.23       197\n",
      "   macro avg       0.03      0.14      0.05       197\n",
      "weighted avg       0.05      0.23      0.08       197\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\internenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\internenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\internenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Evaluation\n",
    "test_outputs = model(X_test)\n",
    "_, predicted_labels = torch.max(test_outputs, 1)\n",
    "\n",
    "predicted_labels = [emotions[label] for label in predicted_labels]\n",
    "true_labels = [emotions[label] for label in y_test.numpy()]\n",
    "\n",
    "\n",
    "# predicted_labels = [emotions[label] for label in predicted_labels]\n",
    "# true_labels = [emotions[label] for label in y_test]\n",
    "classification_metrics = classification_report(true_labels, predicted_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_metrics)\n",
    "\n",
    "# Step 5: Load and Process a Test Image\n",
    "test_image_path = \"images.jpeg\"  # Provide the path to the test image\n",
    "# test_image_path = \"ElonMusk.jpg\"  # Provide the path to the test image\n",
    "\n",
    "# Load and preprocess the test image\n",
    "test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "test_image = cv2.resize(test_image, (img_width, img_height))\n",
    "test_image = torch.from_numpy(test_image).unsqueeze(0).unsqueeze(1).float()\n",
    "\n",
    "# Make prediction for the test image\n",
    "test_output = model(test_image)\n",
    "_, predicted_label = torch.max(test_output, 1)\n",
    "\n",
    "predicted_emotion = emotions[predicted_label.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b561b944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion for the Test Image: surprise\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Predicted Emotion for the Test Image:\", predicted_emotion)\n"
   ]
  },
  {
   "attachments": {
    "images.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ae627cc7",
   "metadata": {},
   "source": [
    "![images.jpeg](attachment:images.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
